# Copyright 2015 Confluent Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
# in compliance with the License. You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software distributed under the License
# is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
# or implied. See the License for the specific language governing permissions and limitations under
# the License.

# Generic properties
# ==================

# Globally unique name to use for this connector.
name=

# Name or alias of the class for this connector. Must be a subclass of org.apache.kafka.connect.connector.Connector
# If the connector is org.apache.kafka.connect.file.FileStreamSinkConnector, you can either specify this full name
# or use "FileStreamSink" or "FileStreamSinkConnector" to make the configuration a bit shorter
connector.class=

# Maximum number of tasks to use for this connector.
tasks.max=1


# Connector-specific properties
# =============================

# JDBC connection URL for the database to load.
connection.url=

# The mode for updating a table each time it is polled. Options include:
#  * bulk - perform a bulk load of the entire table each time it is polled
#  * incrementing - use a strictly incrementing column on each table to
# detect only new rows. Note that this will not detect modifications or
# deletions of existing rows.\n"
#  * timestamp - use a timestamp (or timestamp-like) column to detect new and modified
# rows. This assumes the column is updated with each write, and that values are
# monotonically incrementing, but not necessarily unique.
#   * timestamp+incrementing - use two columns, a timestamp column that detects new and
# modified rows and a strictly incrementing column which provides a globally unique ID for
# updates so each row can be assigned a unique stream offset.
mode=

# Frequency in ms to poll for new data in each table.
# poll.interval.ms=

# Frequency in ms to poll for new or removed tables, which may result in updated task
# configurations to start polling for data in added tables or stop polling for data in
# removed tables.
#table.poll.interval.ms=

# The name of the strictly incrementing column to use to detect new rows. Any empty value
# indicates the column should be autodetected by looking for an auto-incrementing column.
# This column may not be nullable.
incrementing.column.name=

# The name of the timestamp column to use to detect new or modified rows. This column may
# not be nullable.
timestamp.column.name=

# Prefix to prepend to table names to generate the name of the Kafka topic to publish data
# to, or in the case of a custom query, the full name of the topic to publish to.
topic.prefix=

# List of tables to include in copying. If specified, table.blacklist may not be set.
table.whitelist=

# List of tables to exclude from copying. If specified, table.whitelist may not be set.
# table.blacklist=

# By default, the JDBC connector will validate that all incrementing and timestamp tables have NOT NULL set for
# the columns being used as their ID/timestamp. If the tables don't, JDBC connector will fail to start. Setting
# this to false will disable these checks.
# validate.non.null=

# If specified, the query to perform to select new or updated rows. Use this setting if you
# want to join tables, select subsets of columns in a table, or filter data. If used, this
# connector will only copy data using this query -- whole-table copying will be disabled.
# Different query modes may still be used for incremental updates, but in order to
# properly construct the incremental query, it must be possible to append a WHERE clause
# to this query (i.e. no WHERE clauses may be used). If you use a WHERE clause, it must
# handle incremental queries itself.
# query=