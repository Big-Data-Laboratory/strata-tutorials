# Copyright 2015 Confluent Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
# in compliance with the License. You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software distributed under the License
# is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
# or implied. See the License for the specific language governing permissions and limitations under
# the License.

## Generic properties
## ==================

## Globally unique name to use for this connector.
name=

## Name or alias of the class for this connector. Must be a subclass of org.apache.kafka.connect.connector.Connector
## If the connector is org.apache.kafka.connect.file.FileStreamSinkConnector, you can either specify this full name
## or use "FileStreamSink" or "FileStreamSinkConnector" to make the configuration a bit shorter
connector.class=

## Maximum number of tasks to use for this connector.
tasks.max=1

## Topics
topics=taxis_jdbc_yellow_cab_trips


## Connector-specific properties
## =============================

# The HDFS connection URL. This configuration has the format of hdfs:://hostname:port and
# specifies the HDFS to export data to.
hdfs.url=hdfs://localhost:9000

## The Hadoop configuration directory.
# hadoop.conf.dir=

## The Hadoop home directory.
# hadoop.home.dir=

## The format class to use when writing data to HDFS.
# format.class=

## Number of records written to HDFS before invoking file commits.
flush.size=128

## The time interval in milliseconds to invoke file commits. This configuration ensures that
## file commits are invoked every configured interval. This configuration is useful when data
## ingestion rate is low and the connector didn't write enough messages to commit files.
## The default value -1 means that this feature is disabled.
# rotate.interval.ms=

## Top level HDFS directory to store the data ingested from Kafka.
# topics.dir=

## Top level HDFS directory to store the write ahead logs.
# logs.dir=

## The size of the schema cache used in the Avro converter.
# schema.cache.size=

## The underlying storage layer. The default is HDFS
# storage.class=

## The retry backoff in milliseconds. This config is used to
## notify Kafka connect to retry delivering a message batch or performing recovery in case
## of transient exceptions.
# retry.backoff.ms=

## Hive configuration directory
# hive.conf.dir=

## Hive home directory
# hive.home=

## Configuration indicating whether to integrate with Hive when running the connector.
hive.integration=false

## The database to use when the connector creates tables in Hive.
# hive.database=

## The Hive metastore URIs, can be IP address or fully-qualified domain name
## and port of the metastore host.
hive.metastore.uris=thrift://localhost:9083


## Clean shutdown timeout. This makes sure that asynchronous Hive metastore updates are
## completed during connector shutdown.
# shutdown.timeout.ms=

## The schema compatibility rule to use when the connector is observing schema changes. The
## supported configurations are NONE, BACKWARD, FORWARD and FULL.
schema.compatibility=BACKWARD

## The partitioner to use when writing data to HDFS. You can use ``DefaultPartitioner``,
## which preserves the Kafka partitions; ``FieldPartitioner``, which partitions the data to
## different directories according to the value of the partitioning field specified
## in ``partition.field.name``; ``TimebasedPartitioner``, which partitions data
## according to the time ingested to HDFS.
partitioner.class=io.confluent.connect.hdfs.partitioner.DailyPartitioner

## The name of the partitioning field when FieldPartitioner is used.
partition.field.name=tpep_pickup_datetime

## The duration of a partition milliseconds used by ``TimeBasedPartitioner``.
## The default value -1 means that we are not using ``TimebasedPartitioner``.
partition.duration.ms=3600000

## This configuration is used to set the format of the data directories when partitioning with
## ``TimeBasedPartitioner``. The format set in this configuration converts the Unix timestamp
## to proper directories strings. For example, if you set
## ``path.format='year'=YYYY/'month'=MM/'day'=dd/'hour'=HH/``, the data directories will have
##  the format ``/year=2015/month=12/day=07/hour=15``
#path.format==taxis/YYYY/MM/dd/HH/

## The locale to use when partitioning with ``TimeBasedPartitioner``.
locale=en

## The timezone to use when partitioning with ``TimeBasedPartitioner``.
timezone=America/Los_Angeles

## Width to zero pad offsets in HDFS filenames to if the offsets is too short in order to
## provide fixed width filenames that can be ordered by simple lexicographic sorting.
# filename.offset.zero.pad.width=

## Configuration indicating whether HDFS is using Kerberos for authentication.
# hdfs.authentication.kerberos=

## The principal to use when HDFS is using Kerberos to for authentication.
# connect.hdfs.principal

## The path to the keytab file for the HDFS connector principal. "
## This keytab file should only be readable by the connector user.
# connect.hdfs.keytab=